### Deleting all variables
rm(list=ls())

### Install and load libraries
library(ipred)
library(data.table)
library(caret)
library(Boruta)
library(Hmisc, quietly=TRUE) ## For finding the missing value using 'describe function'

### Set the working directory
setwd("/Users/ritesh/pad-datascience/R/")

### Load data and intial analysis
train <- read.csv("ai/data/train.csv", stringsAsFactors = F)
test <- read.csv("ai/data/test.csv", stringsAsFactors = F)

str(train)
str(test)
names(train)
names(test)
# Description:
# The Ames Housing dataset was retrieved from https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data.
# The dataset represents residential properties in Ames, Iowa from 2006 to 2010. There is a train and a test file.
# The train file has 1460 observations and the test file has 1459 observations. Train dataset contain 81 and test
# dataset contain 80 explanatory variables composed of 46 categorical and 33 continuous variables that describe
# house features such as neighborhood, square footage, number of full bathrooms, and many more. The train file
# contains a response variable column, SalePrice, which is what we will predict in the test set. There is also a
# unique ID for each house sold, but were not used in fitting the models.

##################################################################################
##########  Feature Engineering #################
##################################################################################
## When performing regression, sometimes it makes sense to log-transform the target
## variable when it is skewed
## Importantly, the predictions generated by the final model will also be log-transformed,
## so weâ€™ll need to convert these predictions back to their original form later

## Determine data types in the data set
ATTRS <- names(train)
data_types <- sapply(ATTRS,function(x){class(train[[x]])})
unique_data_types <- unique(data_types)

## Separate attributes by data type
DATA_ATTR_TYPES <- lapply(unique_data_types,function(x){ names(data_types[data_types == x])})
names(DATA_ATTR_TYPES) <- unique_data_types


Num_NA<-sapply(train,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(train),Count=Num_NA)
NA_Count

## function to calculate 'mode'
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

## fix charater missing values
imputeMissingAttributes <- function(df){
  # for character  attributes set missing value
  char_attr <- intersect(names(df),DATA_ATTR_TYPES$character)
  for (x in char_attr){
    print(x)
    missing_count_percentage <- describe(df[, x])$counts[2]/describe(df[, x])$counts[1]
    print(missing_count_percentage)
    if(missing_count_percentage < 0.01){
      df[[x]][is.na(df[[x]])] <- Mode(df[, x])
      #df[[x]] <- factor(df[[x]])
    }
  }
  # for numeric set missing values to median is the mising value percentage is less than 0.01
  num_attr <- intersect(names(df),DATA_ATTR_TYPES$integer)
  for (x in num_attr){
    print(x)
    print(describe(df[, x])$counts[2])
    print(describe(df[, x])$counts[1])
    misingVal <- describe(df[, x])$counts[2]
    nVal <- describe(df[, x])$counts[1]
    missing_count_percentage <- as.integer(misingVal)/as.integer(nVal)
    print(missing_count_percentage)
    if(missing_count_percentage < 0.01){
      df[[x]][is.na(df[[x]])] <- median(df[, x], na.rm=TRUE)
    }
  }
  ### As per the Project Discription NA values are replaced to No pool.
  df$PoolQC[which(is.na(df$PoolQC))]="NoPool"
  ### As per the Project Discription NA values are replaced to NO Fance
  df$Fence[which(is.na(df$Fence))]="NoFence"
  ### As per the Project Discription NA values are replaced to None
  df$MiscFeature[which(is.na(df$MiscFeature))]="None"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageQual[which(is.na(df$GarageQual))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageCond[which(is.na(df$GarageCond))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  ### As per the Project discription NA values are replaced to "No Fireplace"
  df$FireplaceQu[which(is.na(df$FireplaceQu))]="NoFireplace"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageType[which(is.na(df$GarageType))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  
  ### As per the Project discription NA values are replaced to "NoBasement"
  df$BsmtCond[which(is.na(df$BsmtCond))]="NoBasement"
  df$BsmtExposure[which(is.na(df$BsmtExposure))]="NoBasement"
  df$BsmtQual[which(is.na(df$BsmtQual))]="NoBasement"
  df$BsmtFinType2[which(is.na(df$BsmtFinType2))]="NoBasement"
  df$BsmtFinType1[which(is.na(df$BsmtFinType1))]="NoBasement"
  #### Data Having 93% of NA'S, but replacing the values as "Noalley"
  df$Alley[is.na(df$Alley)]="Noalley"
  
  #### LotFrontage and GarageYrblt is Major missing Values in data
  #### and can be replaced with bagImpute using Propress
  df_preprocess = preProcess(df[,c("LotFrontage","GarageYrBlt")],method="bagImpute")
  summary(df_preprocess) 
  df_pred= predict(df_preprocess,df[,c("LotFrontage","GarageYrBlt")],type="class")
  df$LotFrontage = df_pred$LotFrontage
  df$GarageYrBlt = df_pred$GarageYrBlt
  
  ##return 'df'
  df
}

train.df <- imputeMissingAttributes(train)

Num_NA <- sapply(train.df,function(y)length(which(is.na(y)==T)))
NA_Count <- data.frame(Item=colnames(train),Count=Num_NA)
NA_Count
#############################################################################################
########## Features selection (using Boruta) #############
#############################################################################################
# Clear variables
#rm(list=ls())

# Load data afresh 
# retrive data for analysis
#sample.df <- read.csv("ai/data/train.csv", stringsAsFactors = F)
#train.raw <- read.csv("ai/data/train.csv", stringsAsFactors = F)
#test.raw <- read.csv("ai/data/test.csv", stringsAsFactors = F)

# extract only candidate feature names i.e exclude 'ID' and lable column
candidate.features <- setdiff(names(train.df),c("Id","SalePrice"))
data.type <- sapply(candidate.features,function(x){class(train.df[[x]])})
table(data.type)

# Determine data types
explanatory.attributes <- setdiff(names(train.df),c("Id","SalePrice"))
data.classes <- sapply(explanatory.attributes,function(x){class(train.df[[x]])})

# categorize data types in the data set?
unique.classes <- unique(data.classes)

attr.data.types <- lapply(unique.classes,function(x){names(data.classes[data.classes==x])})
names(attr.data.types) <- unique.classes

# Prepare data set for Boruta analysis. As Boruta uses Random forest, missng data needs to be fixed
# pull out the response variable
response <- train.df$SalePrice

# remove identifier and response variables
train.df <- train.df[candidate.features]

# for numeric set missing values to -1 for purposes of the random forest run
for (x in attr.data.types$integer){
  train.df[[x]][is.na(sample.df[[x]])] <- -1
}
# for charater set missing values to *MISSING* for purposes of the random forest run
for (x in attr.data.types$character){
  train.df[[x]][is.na(train.df[[x]])] <- "*MISSING*"
}

### Run Boruta Analysis ###
set.seed(13)
bor.results <- Boruta(train.df,response,
                      maxRuns=101,
                      doTrace=0)

# Print Boruta result
bor.results

## Bucketing attributes as 'Confirmed', 'Tentative' and 'Rejected' :
CONFIRMED_ATTR <- getSelectedAttributes(bor.results, withTentative = F)
CONFIRMED_ATTR_WITH_TENTATIVE <- getSelectedAttributes(bor.results, withTentative = T)
TENTATIVE_ATTR <- CONFIRMED_ATTR_WITH_TENTATIVE[52:length(CONFIRMED_ATTR_WITH_TENTATIVE)]
REJECTED_ATTR <- names(train.df)[!(names(train.df) %in% CONFIRMED_ATTR_WITH_TENTATIVE)]
PREDICTOR_ATTR <- c(CONFIRMED_ATTR,TENTATIVE_ATTR,REJECTED_ATTR)

# create folds for training
set.seed(13)
data_folds <- createFolds(train$SalePrice, k=5)

### plot the boruta variable importance chart.
# Blue boxplots correspond to minimal, average and maximum Z score of a shadow attribute. Red, yellow and green boxplots
# represent Z scores of rejected, tentative and confirmed attributes respectively.
plot(bor.results, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(bor.results$ImpHistory),function(i)bor.results$ImpHistory[is.finite(bor.results$ImpHistory[,i]),i])
names(lz) <- colnames(bor.results$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(bor.results$ImpHistory), cex.axis = 0.7)

attStats(bor.results)

#########   ------  ###############
#final.boruta <- TentativeRoughFix(bor.results)
#plot(final.boruta, xlab = "", xaxt = "n")
#lz<-lapply(1:ncol(final.boruta$ImpHistory),function(i)final.boruta$ImpHistory[is.finite(final.boruta$ImpHistory[,i]),i])
#names(lz) <- colnames(final.boruta$ImpHistory)
#Labels <- sort(sapply(lz,median))
#axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(final.boruta$ImpHistory), cex.axis = 0.7)
#########   ------  ###############

library(scales)
ggplot(train, aes(x=SalePrice)) + geom_histogram(col = 'white') + theme_light() +scale_x_continuous(labels = comma)
summary(train[,c("SalePrice")])
#Normalize distribution
ggplot(train, aes(x=log(SalePrice+1))) + geom_histogram(col = 'white') + theme_light()
