---
title: "HousePriceAssignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Deleting all variables
```{r clear workspace}
rm(list=ls())
```
## Install and load libraries
```{r, warning=FALSE}
library(ipred)
library(data.table)
library(caret)
library(Boruta)
library(Hmisc, quietly=TRUE) ## For finding the missing value using 'describe function'
require(plyr) ## for 'llply' function
library(knitr)
library(dplyr)
library(xgboost)
library(ranger)
library(nnet)
library(Metrics)
library(gbm)
library(h2o)
```
## Set the working directory & Load data and intial analysis
```{r}
setwd("/Users/ritesh/pad-datascience/R/")

### Load data and intial analysis
train.df <- read.csv("ai/data/train.csv", stringsAsFactors = F)
test.df <- read.csv("ai/data/test.csv", stringsAsFactors = F)

#str(train.df)
#str(test.df)
#names(train.df)
#names(test.df)
```
# Description:
The Ames Housing dataset was retrieved from https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data.
The dataset represents residential properties in Ames, Iowa from 2006 to 2010. There is a train and a test file.
The train file has 1460 observations and the test file has 1459 observations. Train dataset contain 81 and test
dataset contain 80 explanatory variables composed of 46 categorical and 33 continuous variables that describe
house features such as neighborhood, square footage, number of full bathrooms, and many more. The train file
contains a response variable column, SalePrice, which is what we will predict in the test set. There is also a
unique ID for each house sold, but were not used in fitting the models.

##  Feature Engineering 
When performing regression, sometimes it makes sense to log-transform the target
variable when it is skewed
Importantly, the predictions generated by the final model will also be log-transformed,
so we'll need to convert these predictions back to their original form later
```{r}
## Determine data types in the data set
ATTRS <- names(train.df)
data_types <- sapply(ATTRS,function(x){class(train.df[[x]])})
unique_data_types <- unique(data_types)

## Separate attributes by data type
DATA_ATTR_TYPES <- lapply(unique_data_types,function(x){ names(data_types[data_types == x])})
names(DATA_ATTR_TYPES) <- unique_data_types

Num_NA<-sapply(train.df,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(train.df),Count=Num_NA)
NA_Count
```
## Fix missing values
```{r}
## function to calculate 'mode'
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

imputeMissingAttributes <- function(df){
  
  # for character  attributes set missing value
  char_attr <- intersect(names(df),DATA_ATTR_TYPES$character)
  for (x in char_attr){
    print(x)
    print(describe(df[, x])$counts[2])
    print(describe(df[, x])$counts[1])
    misingVal <- describe(df[, x])$counts[2]
    nVal <- describe(df[, x])$counts[1]
    missing_count_percentage <- as.integer(misingVal)/as.integer(nVal)
    print(missing_count_percentage)
    if(missing_count_percentage < 0.015){
      df[[x]][is.na(df[[x]])] <- Mode(df[, x])
      #df[[x]] <- factor(df[[x]])
    }
  }
  # for numeric set missing values to median is the mising value percentage is less than 0.015
  num_attr <- intersect(names(df),DATA_ATTR_TYPES$integer)
  for (x in num_attr){
    print(x)
    print(describe(df[, x])$counts[2])
    print(describe(df[, x])$counts[1])
    misingVal <- describe(df[, x])$counts[2]
    nVal <- describe(df[, x])$counts[1]
    missing_count_percentage <- as.integer(misingVal)/as.integer(nVal)
    print(missing_count_percentage)
    if(missing_count_percentage < 0.015){
      df[[x]][is.na(df[[x]])] <- median(df[, x], na.rm=TRUE)
    }
  }
  ### As per the Project Discription NA values are replaced to No pool.
  df$PoolQC[which(is.na(df$PoolQC))]="NoPool"
  ### As per the Project Discription NA values are replaced to NO Fance
  df$Fence[which(is.na(df$Fence))]="NoFence"
  ### As per the Project Discription NA values are replaced to None
  df$MiscFeature[which(is.na(df$MiscFeature))]="None"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageQual[which(is.na(df$GarageQual))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageCond[which(is.na(df$GarageCond))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  ### As per the Project discription NA values are replaced to "No Fireplace"
  df$FireplaceQu[which(is.na(df$FireplaceQu))]="NoFireplace"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageType[which(is.na(df$GarageType))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  df$GarageYrBlt[which(df$GarageYrBlt==2207)]=2007
  ### As per the Project discription NA values are replaced to "NoBasement"
  df$BsmtCond[which(is.na(df$BsmtCond))]="NoBasement"
  df$BsmtExposure[which(is.na(df$BsmtExposure))]="NoBasement"
  df$BsmtQual[which(is.na(df$BsmtQual))]="NoBasement"
  df$BsmtFinType2[which(is.na(df$BsmtFinType2))]="NoBasement"
  df$BsmtFinType1[which(is.na(df$BsmtFinType1))]="NoBasement"
  #### Data Having 93% of NA'S, but replacing the values as "Noalley"
  df$Alley[is.na(df$Alley)]="Noalley"
  
  #### LotFrontage and GarageYrblt is Major missing Values in data
  #### and can be replaced with bagImpute using Propress
  df_preprocess = preProcess(df[,c("LotFrontage","GarageYrBlt")],method="bagImpute")
  summary(df_preprocess) 
  df_pred= predict(df_preprocess,df[,c("LotFrontage","GarageYrBlt")],type="class")
  df$LotFrontage = df_pred$LotFrontage
  df$GarageYrBlt = df_pred$GarageYrBlt
  
  ##return 'df'
  df
}

train_df <- imputeMissingAttributes(train.df)

#View(train_df)
Num_NA_train <- sapply(train_df,function(y)length(which(is.na(y)==T)))
NA_Count_train <- data.frame(Item=colnames(train_df),Count=Num_NA_train)
NA_Count_train

test_df <- imputeMissingAttributes(test.df)

Num_NA_test <- sapply(test_df,function(y)length(which(is.na(y)==T)))
NA_Count_test <- data.frame(Item=colnames(test_df),Count=Num_NA_test)
NA_Count_test

View(train_df)
```
######## Features selection (using Boruta) 
```{r}
extractFeatures <- function(train.dataframe){
  # extract only candidate feature names i.e exclude 'ID' and lable column
  candidate.features <- setdiff(names(train.dataframe),c("Id","SalePrice"))
  data.type <- sapply(candidate.features,function(x){class(train.dataframe[[x]])})
  table(data.type)
  # Determine data types
  explanatory.attributes <- setdiff(names(train.dataframe),c("Id","SalePrice"))
  data.classes <- sapply(explanatory.attributes,function(x){class(train.dataframe[[x]])})
  
  # categorize data types in the data set?
  unique.classes <- unique(data.classes)
  
  attr.data.types <- lapply(unique.classes,function(x){names(data.classes[data.classes==x])})
  names(attr.data.types) <- unique.classes
  
  # Prepare data set for Boruta analysis. As Boruta uses Random forest, missng data needs to be fixed
  # pull out the response variable
  response <- train.dataframe$SalePrice
  
  # remove identifier and response variables
  train.dataframe <- train.dataframe[candidate.features]
  
  ### Run Boruta Analysis ###
  set.seed(13)
  bor.results <- Boruta(train.dataframe,response,
                       maxRuns=201,
                       doTrace=0)
  
  # Print Boruta result
  print(bor.results)
  ## Bucketing attributes as 'Confirmed', 'Tentative' and 'Rejected' :
  CONFIRMED_ATTR <- getSelectedAttributes(bor.results, withTentative = F)
  CONFIRMED_ATTR_WITH_TENTATIVE <- getSelectedAttributes(bor.results, withTentative = T)
  TENTATIVE_ATTR <- CONFIRMED_ATTR_WITH_TENTATIVE[which(!CONFIRMED_ATTR_WITH_TENTATIVE %in% CONFIRMED_ATTR)]
  REJECTED_ATTR <- names(train.dataframe)[!(names(train.dataframe) %in% CONFIRMED_ATTR_WITH_TENTATIVE)]
  PREDICTOR_ATTR <- c(CONFIRMED_ATTR,TENTATIVE_ATTR,REJECTED_ATTR)

  return(list(
   CONFIRMED_ATTR = CONFIRMED_ATTR,
   TENTATIVE_ATTR = TENTATIVE_ATTR,
   REJECTED_ATTR = REJECTED_ATTR,
   PREDICTOR_ATTR = PREDICTOR_ATTR,
   BOR_RESULT = bor.results))
  
  # CONFIRMED_ATTR <- c(
  #   "MSSubClass", "MSZoning", "LotFrontage", "LotArea", "LotShape", "LandContour", "Neighborhood", "BldgType",
  #   "HouseStyle", "OverallQual", "OverallCond", "YearBuilt", "YearRemodAdd", "Exterior1st", "Exterior2nd", "MasVnrArea",
  #   "ExterQual", "Foundation", "BsmtQual", "BsmtExposure", "BsmtFinType1", "BsmtFinSF1", "BsmtUnfSF", "TotalBsmtSF",
  #   "HeatingQC", "CentralAir", "Electrical", "X1stFlrSF", "X2ndFlrSF", "GrLivArea", "BsmtFullBath", "FullBath",
  #   "HalfBath", "BedroomAbvGr", "KitchenAbvGr", "KitchenQual", "TotRmsAbvGrd", "Functional", "Fireplaces", "FireplaceQu",
  #   "GarageType", "GarageYrBlt", "GarageFinish", "GarageCars", "GarageArea", "GarageQual", "GarageCond", "PavedDrive",
  #   "WoodDeckSF", "OpenPorchSF")
  # TENTATIVE_ATTR <- c(
  #   "Alley", "LandSlope", "Condition1", "RoofStyle", "MasVnrType", "EnclosedPorch", "SaleCondition")
  # REJECTED_ATTR <- c(
  #   "Street", "Utilities", "LotConfig", "Condition2", "RoofMatl", "ExterCond", "BsmtCond",
  #   "BsmtFinType2","BsmtFinSF2", "Heating", "LowQualFinSF", "BsmtHalfBath", "X3SsnPorch", "ScreenPorch",
  #   "PoolArea", "PoolQC", "Fence", "MiscFeature", "MiscVal", "MoSold", "YrSold", "SaleType")
  # return(list(
  #   CONFIRMED_ATTR = CONFIRMED_ATTR,
  #   TENTATIVE_ATTR = TENTATIVE_ATTR,
  #   REJECTED_ATTR = REJECTED_ATTR))
  
}

attr_list <- extractFeatures(train_df)
# create folds for training
set.seed(13)
data_folds <- createFolds(train_df$SalePrice, k=5)

### plot the boruta variable importance chart.
# Blue boxplots correspond to minimal, average and maximum Z score of a shadow attribute. Red, yellow and green boxplots
# represent Z scores of rejected, tentative and confirmed attributes respectively.
plot(attr_list$BOR_RESULT, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(attr_list$BOR_RESULT$ImpHistory),function(i)attr_list$BOR_RESULT$ImpHistory[is.finite(attr_list$BOR_RESULT$ImpHistory[,i]),i])
names(lz) <- colnames(attr_list$BOR_RESULT$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(attr_list$BOR_RESULT$ImpHistory), cex.axis = 0.7)

attStats(attr_list$BOR_RESULT)

#########   ------  ###############
# final.boruta <- TentativeRoughFix(attr_list$BOR_RESULT)
# plot(final.boruta, xlab = "", xaxt = "n")
# lz<-lapply(1:ncol(final.boruta$ImpHistory),function(i)final.boruta$ImpHistory[is.finite(final.boruta$ImpHistory[,i]),i])
# names(lz) <- colnames(final.boruta$ImpHistory)
# Labels <- sort(sapply(lz,median))
# axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(final.boruta$ImpHistory), cex.axis = 0.7)
#########   ------  ###############

```
## Creating boolean columns from factor variables
```{r}
## Feature Set(Boruta Confirmed and tentative Attributes) with binomial column from factor  
featureSet <- function(train.df, test.df) { 
  train.id <- train_df$Id
  test.id <- test_df$Id
  if (class(train_df$SalePrice) != "NULL") {
    y <- log(train_df$SalePrice)
  } else {
    y <- NULL
  }
  test_df$SalePrice = 0
  combined.df <- rbind(train_df, test_df)
  
  #predictor_vars <- c(attr_list$CONFIRMED_ATTR,attr_list$TENTATIVE_ATTR)
  predictor_vars <- names(combined.df)
  #predictors <- combined.df[c("Id", predictor_vars)]
  predictors <- combined.df
  # for character  atributes set missing value
  char_attr <- intersect(predictor_vars, DATA_ATTR_TYPES$character)
  for (x in char_attr){
    predictors[[x]] <- factor(predictors[[x]])
  }
  exploded.df <- 
    data.frame( 
      model.matrix(
        ~.+0, 
        data=predictors, 
        contrasts.arg=sapply(predictors[, char_attr], contrasts, contrasts=F)))
  dim(exploded.df)
  #View(exploded.df)
  # Spilt to Train and Test Data
  exploded.train.df <- exploded.df[exploded.df$Id < 1461, ]
  exploded.test.df <- exploded.df[exploded.df$Id > 1460, ]
  #View(exploded.test.df)

 return(
   list(
     train.List = list(id=train.id,y=y,predictors = exploded.train.df), 
     test.List = list(id=test.id, predictors = exploded.test.df ),
     combined.List = list(predictors = exploded.df)))
}
featureList <- featureSet(train_df, test_df)
#trainFeatureList$predictors
dim(featureList$train.List$predictors)
#testFeatureList$predictors
dim(featureList$test.List$predictors)
setdiff(names(featureList$train.List$predictors), names(featureList$test.List$predictors) )
#View(featureList$train.List$predictors)

```
## Bargraph to dipict distribution of output
```{r}
library(scales)
ggplot(train.df, aes(x=SalePrice)) + geom_histogram(col = 'white') + theme_light() +scale_x_continuous(labels = comma)
summary(train.df[,c("SalePrice")])
#Normalize distribution
ggplot(train.df, aes(x=log(SalePrice+1))) + geom_histogram(col = 'white') + theme_light()
```
## Prediction with H2o package
```{r}
### To launch the H2O cluster, 
localH2O <- h2o.init(nthreads = -1)

# h2o.train.df <- featureList$train.List$predictors
# h2o.train.df$SalePrice <- featureList$train.List$y
# h2o.test.df <- featureList$test.List$predictors
h2o.train.df <- train_df[c(attr_list$CONFIRMED_ATTR,attr_list$TENTATIVE_ATTR)]
View(h2o.train.df)
h2o.train.df$SalePrice <- log(train_df$SalePrice)
h2o.test.df <- test_df[c(attr_list$CONFIRMED_ATTR,attr_list$TENTATIVE_ATTR)]
#data to h2o cluster
train.h2o <- as.h2o(h2o.train.df)
test.h2o <- as.h2o(h2o.test.df)
#check column index number
colnames(train.h2o)
#dependent variable (Purchase)
y.dep <- 58
#independent variables (dropping ID variables)
x.indep <- c(1:57)

#deep learning model------------------------------------ 
dlearning.model <- h2o.deeplearning(y = y.dep,
                                    x = x.indep,
                                    training_frame = train.h2o,
                                    epoch = 1000,
                                    hidden = c(1000,1000),
                                    activation = "RectifierWithDropout",
                                    seed = 1122
)

h2o.performance(dlearning.model)
#make predictions
predict.dl2 <- as.data.frame(h2o.predict(dlearning.model, test.h2o))
sub_dl <- data.frame(Id = test_df$Id, SalePrice  = exp(predict.dl2$predict))
write.csv(sub_dl, file = "/Users/ritesh/Downloads/dl_submission1.csv", row.names = F)

h2o.shutdown(prompt = TRUE)

```


