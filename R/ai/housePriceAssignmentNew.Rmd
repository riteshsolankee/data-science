---
title: "HousePriceAssignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## Deleting all variables
```{r clear workspace}
rm(list=ls())
```
## Install and load libraries
```{r, message=FALSE, warning=FALSE}
library(ipred)
library(data.table)
library(caret)
library(Boruta)
library(Hmisc, quietly=TRUE) ## For finding the missing value using 'describe function'
require(plyr) ## for 'llply' function
library(knitr)
library(dplyr)
library(xgboost)
library(ranger)
library(nnet)
library(Metrics)
library(gbm)
library(moments)
library(h2o)
```
## Set the working directory & Load data and intial analysis
```{r}
setwd("/Users/ritesh/pad-datascience/R/")

### Load data and intial analysis
train.df <- read.csv("ai/data/train.csv", stringsAsFactors = F)
test.df <- read.csv("ai/data/test.csv", stringsAsFactors = F)

#str(train.df)
#str(test.df)
#names(train.df)
#names(test.df)
```
# Description:
The Ames Housing dataset was retrieved from https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data.
The dataset represents residential properties in Ames, Iowa from 2006 to 2010. There is a train and a test file.
The train file has 1460 observations and the test file has 1459 observations. Train dataset contain 81 and test
dataset contain 80 explanatory variables composed of 46 categorical and 33 continuous variables that describe
house features such as neighborhood, square footage, number of full bathrooms, and many more. The train file
contains a response variable column, SalePrice, which is what we will predict in the test set. There is also a
unique ID for each house sold, but were not used in fitting the models.

##  Feature Engineering 
When performing regression, sometimes it makes sense to log-transform the target
variable when it is skewed
Importantly, the predictions generated by the final model will also be log-transformed,
so we'll need to convert these predictions back to their original form later

# combine train and test data for preprocessing
```{r}
# combine train and test data for preprocessing
nrow(train.df)
all_data <- rbind(train.df[,1:(ncol(train.df) - 1)],
                  test.df[,1:(ncol(train.df)-1)])
dim(all_data)
```
```{r}
## Determine data types in the data set
ATTRS <- names(all_data)
data_types <- sapply(ATTRS,function(x){class(train.df[[x]])})
unique_data_types <- unique(data_types)

## Separate attributes by data type
DATA_ATTR_TYPES <- lapply(unique_data_types,function(x){ names(data_types[data_types == x])})
names(DATA_ATTR_TYPES) <- unique_data_types

Num_NA<-sapply(all_data,function(y)length(which(is.na(y)==T)))
NA_Count<- data.frame(Item=colnames(all_data),Count=Num_NA)
NA_Count
```
## Fix missing values
```{r}
## function to calculate 'mode'
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

imputeMissingAttributes <- function(df){
  # df <- all_data
  # for character  attributes set missing value
  char_attr <- intersect(names(df),DATA_ATTR_TYPES$character)
  for (x in char_attr){
    print(x)
    print(describe(df[, x])$counts[2])
    print(describe(df[, x])$counts[1])
    misingVal <- describe(df[, x])$counts[2]
    nVal <- describe(df[, x])$counts[1]
    missing_count_percentage <- as.integer(misingVal)/as.integer(nVal)
    print(missing_count_percentage)
    if(missing_count_percentage < 0.015){
      df[[x]][is.na(df[[x]])] <- Mode(df[, x])
      #df[[x]] <- factor(df[[x]])
    }
  }
  # for numeric set missing values to median is the mising value percentage is less than 0.015
  num_attr <- intersect(names(df),DATA_ATTR_TYPES$integer)
  for (x in num_attr){
    print(x)
    print(describe(df[, x])$counts[2])
    print(describe(df[, x])$counts[1])
    misingVal <- describe(df[, x])$counts[2]
    nVal <- describe(df[, x])$counts[1]
    missing_count_percentage <- as.integer(misingVal)/as.integer(nVal)
    print(missing_count_percentage)
    if(missing_count_percentage < 0.015){
      df[[x]][is.na(df[[x]])] <- median(df[, x], na.rm=TRUE)
    }
  }
  ### As per the Project Discription NA values are replaced to No pool.
  df$PoolQC[which(is.na(df$PoolQC))]="NoPool"
  ### As per the Project Discription NA values are replaced to NO Fance
  df$Fence[which(is.na(df$Fence))]="NoFence"
  ### As per the Project Discription NA values are replaced to None
  df$MiscFeature[which(is.na(df$MiscFeature))]="None"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageQual[which(is.na(df$GarageQual))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageCond[which(is.na(df$GarageCond))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  ### As per the Project discription NA values are replaced to "No Fireplace"
  df$FireplaceQu[which(is.na(df$FireplaceQu))]="NoFireplace"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageType[which(is.na(df$GarageType))]="NoGarage"
  ### As per the Project Discription NA values are replaced to No Grage
  df$GarageFinish[which(is.na(df$GarageFinish))]="NoGarage"
  df$GarageYrBlt[which(df$GarageYrBlt==2207)]=2007
  ### As per the Project discription NA values are replaced to "NoBasement"
  df$BsmtCond[which(is.na(df$BsmtCond))]="NoBasement"
  df$BsmtExposure[which(is.na(df$BsmtExposure))]="NoBasement"
  df$BsmtQual[which(is.na(df$BsmtQual))]="NoBasement"
  df$BsmtFinType2[which(is.na(df$BsmtFinType2))]="NoBasement"
  df$BsmtFinType1[which(is.na(df$BsmtFinType1))]="NoBasement"
  #### Data Having 93% of NA'S, but replacing the values as "Noalley"
  df$Alley[is.na(df$Alley)]="Noalley"
  
  #### LotFrontage and GarageYrblt is Major missing Values in data
  #### and can be replaced with bagImpute using PreProcess
  df_preprocess = preProcess(df[,c("LotFrontage","GarageYrBlt")], method="bagImpute")
  summary(df_preprocess) 
  df_pred = predict(df_preprocess,df[,c("LotFrontage","GarageYrBlt")],type="class")
  df$LotFrontage = df_pred$LotFrontage
  df$GarageYrBlt = df_pred$GarageYrBlt
  
  ##return 'df'
  df
}

all.data <- imputeMissingAttributes(all_data)
Num_NA_all <- sapply(all.data,function(y)length(which(is.na(y)==T)))
NA_Count_all <- data.frame(Item=colnames(all.data),Count=Num_NA_all)
NA_Count_all

dim(all.data)

```
## Check if the response column is normally distrinuted. If not, then use 'log' function 
```{r}
# get data frame of SalePrice and log(SalePrice + 1) for plotting
df <- rbind(data.frame(version="log(price+1)",x=log(train.df$SalePrice)),
            data.frame(version="price",x=train.df$SalePrice))

# plot histogram
ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x))
```
## Check if numerical variables are normally distributed or not. If not, normalize it using 'log' function 
```{r}
# transform SalePrice target to log form
train.df$SalePrice <- log(train.df$SalePrice)

#View(all.data)
# for numeric feature with excessive skewness, perform log transformation
# first get data type for each feature
feature_classes <- sapply(names(all.data),function(x){class(all.data[[x]])})
numeric_feats <-names(feature_classes[feature_classes != "character"])

# determine skew for each numeric feature
skewed_feats <- sapply(numeric_feats,function(x){skewness(all.data[[x]],na.rm=TRUE)})

# keep only features that exceed a threshold for skewness
skewed_feats <- skewed_feats[skewed_feats > 0.75]

# transform excessively skewed features with log(x + 1)
for(x in names(skewed_feats)) {
  all.data[[x]] <- log(all.data[[x]] + 1)
}

```
## Creating dummy variables from categorical attributes
```{r }
# get names of categorical features
categorical_feats <- names(feature_classes[feature_classes == "character"])

# use caret dummyVars function for hot one encoding for categorical features
dummies <- dummyVars(~.,all.data[categorical_feats])
categorical_1_hot <- predict(dummies,all.data[categorical_feats])
categorical_1_hot[is.na(categorical_1_hot)] <- 0  #for any level that was NA, set to zero

```
## reconstruct all_data with pre-processed data
```{r In_9}
all.data <- cbind(all.data[numeric_feats],categorical_1_hot)

# create data for training and test
X_train <- all.data[1:nrow(train.df),]
X_test <- all.data[(nrow(train.df)+1):nrow(all.data),]
y <- train.df$SalePrice

dim(X_train)
```

## Features selection (using Boruta) 
```{r}
extractFeatures <- function(train.dataframe, response){
  #train.dataframe <- X_train
  # extract only candidate feature names i.e exclude 'ID' and lable column
  candidate.features <- setdiff(names(train.dataframe),c("Id","SalePrice"))
  data.type <- sapply(candidate.features,function(x){class(train.dataframe[[x]])})
  table(data.type)
  # Determine data types
  explanatory.attributes <- setdiff(names(train.dataframe),c("Id","SalePrice"))
  data.classes <- sapply(explanatory.attributes,function(x){class(train.dataframe[[x]])})
  
  # categorize data types in the data set?
  unique.classes <- unique(data.classes)
  
  attr.data.types <- lapply(unique.classes,function(x){names(data.classes[data.classes==x])})
  names(attr.data.types) <- unique.classes
  
  # Prepare data set for Boruta analysis. As Boruta uses Random forest, missng data needs to be fixed before applying the 
  # algorithm
  
  # remove identifier and response variables
  train.dataframe <- train.dataframe[candidate.features]
  
  ### Run Boruta Analysis ###
  set.seed(13)
  bor.results <- Boruta(
                        train.dataframe,
                        response,
                        maxRuns=201,
                        doTrace=0)
  
  # Print Boruta result
  print(bor.results)
  ## Bucketing attributes as 'Confirmed', 'Tentative' and 'Rejected' :
  CONFIRMED_ATTR <- getSelectedAttributes(bor.results, withTentative = F)
  CONFIRMED_ATTR_WITH_TENTATIVE <- getSelectedAttributes(bor.results, withTentative = T)
  TENTATIVE_ATTR <- CONFIRMED_ATTR_WITH_TENTATIVE[which(!CONFIRMED_ATTR_WITH_TENTATIVE %in% CONFIRMED_ATTR)]
  REJECTED_ATTR <- names(train.dataframe)[!(names(train.dataframe) %in% CONFIRMED_ATTR_WITH_TENTATIVE)]
  PREDICTOR_ATTR <- c(CONFIRMED_ATTR,TENTATIVE_ATTR,REJECTED_ATTR)

  return(list(
   CONFIRMED_ATTR = CONFIRMED_ATTR,
   TENTATIVE_ATTR = TENTATIVE_ATTR,
   REJECTED_ATTR = REJECTED_ATTR,
   PREDICTOR_ATTR = PREDICTOR_ATTR,
   BOR_RESULT = bor.results))
  
}

attr_list <- extractFeatures(X_train, y)
```

## plot the boruta variable importance chart.
Blue boxplots correspond to minimal, average and maximum Z score of a shadow attribute. Red, yellow and green boxplots represent Z scores of rejected, tentative and confirmed attributes respectively.
```{r }
# Blue boxplots correspond to minimal, average and maximum Z score of a shadow attribute. Red, yellow and green boxplots
# represent Z scores of rejected, tentative and confirmed attributes respectively.
plot(attr_list$BOR_RESULT, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(attr_list$BOR_RESULT$ImpHistory),function(i)attr_list$BOR_RESULT$ImpHistory[is.finite(attr_list$BOR_RESULT$ImpHistory[,i]),i])
names(lz) <- colnames(attr_list$BOR_RESULT$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(attr_list$BOR_RESULT$ImpHistory), cex.axis = 0.7)

attStats(attr_list$BOR_RESULT)

#########   ------  ###############
# final.boruta <- TentativeRoughFix(attr_list$BOR_RESULT)
# plot(final.boruta, xlab = "", xaxt = "n")
# lz<-lapply(1:ncol(final.boruta$ImpHistory),function(i)final.boruta$ImpHistory[is.finite(final.boruta$ImpHistory[,i]),i])
# names(lz) <- colnames(final.boruta$ImpHistory)
# Labels <- sort(sapply(lz,median))
# axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(final.boruta$ImpHistory), cex.axis = 0.7)
#########   ------  ###############

```

## Prediction with H2o package
```{r}
### To launch the H2O cluster, 
localH2O <- h2o.init(nthreads = -1)

# h2o.train.df <- featureList$train.List$predictors
# h2o.train.df$SalePrice <- featureList$train.List$y
# h2o.test.df <- featureList$test.List$predictors
h2o.train.df <- X_train[c(attr_list$CONFIRMED_ATTR,attr_list$TENTATIVE_ATTR)]
View(h2o.train.df)
h2o.train.df$SalePrice <- train.df$SalePrice
h2o.test.df <- X_test[c(attr_list$CONFIRMED_ATTR,attr_list$TENTATIVE_ATTR)]
#data to h2o cluster
train.h2o <- as.h2o(h2o.train.df)
test.h2o <- as.h2o(h2o.test.df)
#check column index number
colnames(train.h2o)
#dependent variable (Purchase)
y.dep <- 117
#independent variables (dropping ID variables)
x.indep <- c(1:116)

#deep learning model------------------------------------ 
dlearning.model <- h2o.deeplearning(y = y.dep,
                                    x = x.indep,
                                    training_frame = train.h2o,
                                    epoch = 1500,
                                    hidden = c(1500, 1500, 1500, 1500),
                                    activation = "Rectifier",
                                    seed = 1122,
                                    rate = 0.01,
                                    l1=1e-6,
                                    l2=1e-6,
                                    max_w2=10
                                    
)

h2o.performance(dlearning.model)
#make predictions
predict.dl2 <- as.data.frame(h2o.predict(dlearning.model, test.h2o))
sub_dl <- data.frame(Id = test.df$Id, SalePrice  = exp(predict.dl2$predict))
write.csv(sub_dl, file = "/Users/ritesh/Downloads/dl_submission5.csv", row.names = F)

h2o.shutdown(prompt = TRUE)

```


